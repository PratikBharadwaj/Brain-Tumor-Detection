{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f51f34e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Output Size: torch.Size([2, 1, 5, 5])\n",
      "Real Images Size: torch.Size([2, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Discriminator Output Size:\", real_outputs.size())\n",
    "print(\"Real Images Size:\", real_images.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75825b1",
   "metadata": {},
   "source": [
    "The architecture of the GAN implemented in the provided code is a DCGAN (Deep Convolutional Generative Adversarial Network). DCGAN is a popular and effective variant of GAN that uses deep convolutional neural networks for both the generator and discriminator.\n",
    "\n",
    "DCGAN introduces several key architectural changes compared to the original GAN:\n",
    "\n",
    "1. Convolutional Layers: The generator and discriminator use convolutional layers instead of fully connected layers, making them better suited for processing images.\n",
    "\n",
    "2. Transposed Convolution (ConvTranspose2d): The generator uses transposed convolutional layers (also known as deconvolution) to upscale the random noise (latent vector) into a generated image.\n",
    "\n",
    "3. Batch Normalization: Batch normalization is applied after each convolutional layer in both the generator and discriminator. This helps stabilize and accelerate training.\n",
    "\n",
    "4. Leaky ReLU Activation: Leaky ReLU activation function is used in the discriminator, allowing gradients to flow through when the output is negative.\n",
    "\n",
    "5. Sigmoid Activation: The generator's output is passed through a sigmoid activation function to ensure the generated image pixels are in the range [0, 1].\n",
    "\n",
    "Overall, DCGANs have been found to be very effective for generating high-quality images and are widely used for various image synthesis tasks. The architecture is flexible and can be adapted to different datasets and image sizes by adjusting the network sizes, layers, and hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0b23a4",
   "metadata": {},
   "source": [
    "The error message you encountered indicates that there is not enough memory available to allocate the required tensors. The model architecture you are using, particularly the transposed convolutional layers, requires a significant amount of memory, especially for larger image sizes like 512x512.\n",
    "\n",
    "To address this issue, you can try the following options:\n",
    "\n",
    "1. Reduce the model's capacity: You can decrease the `hidden_size` in the `Generator` class or use fewer layers in the generator and discriminator to reduce the memory footprint.\n",
    "\n",
    "2. Decrease the batch size: Using a smaller batch size in the training loop will require less memory. Try reducing the `batch_size` in the `DataLoader`.\n",
    "\n",
    "3. Use GPU with larger memory: If you have access to a GPU with more memory, you can try running the code on that GPU.\n",
    "\n",
    "4. Resize the input images: If reducing the batch size or model capacity is not feasible, you can resize the input images to a smaller size (e.g., 128x128) during preprocessing. However, this may affect the quality of the generated images.\n",
    "\n",
    "5. Use mixed precision training: If you have a GPU with Tensor Cores (e.g., NVIDIA's Volta and later GPUs), you can use mixed precision training to reduce memory consumption.\n",
    "\n",
    "Remember that working with large images in GANs can be computationally expensive and memory-intensive. If you're working with limited hardware resources, consider starting with smaller image sizes to test your implementation before scaling up to 512x512 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90413c33",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20132659200 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\1397039714.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;31m# Instantiate the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_output_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;31m# Instantiate the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\1397039714.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, z_size, channels, hidden_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m         self.main = nn.Sequential(\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Input: z_size x 1 x 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias, dilation, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mdilation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0moutput_padding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             True, output_padding, groups, bias, padding_mode, **factory_kwargs)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    626\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_padding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             self.weight = Parameter(torch.empty(\n\u001b[0m\u001b[0;32m    135\u001b[0m                 (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n\u001b[0;32m    136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20132659200 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_size, channels, hidden_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: z_size x 1 x 1\n",
    "            nn.ConvTranspose2d(z_size, hidden_size * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 4),\n",
    "            nn.ReLU(True),\n",
    "            # Output: (hidden_size*4) x 4 x 4\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size * 4, hidden_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 2),\n",
    "            nn.ReLU(True),\n",
    "            # Output: (hidden_size*2) x 8 x 8\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size * 2, hidden_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            # Output: hidden_size x 16 x 16\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: channels x 32 x 32 (final image size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    " \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        out = torch.sigmoid(self.fc4(x))  # Use sigmoid activation to ensure output in the range [0, 1]\n",
    "        return out.view(-1, 1, 1, 1)  # Reshape the output to (batch_size, 1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset and transformation\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_list = os.listdir(data_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_folder, self.image_list[idx])\n",
    "        image = Image.open(image_path).convert(\"L\")  # Convert to grayscale image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "data_folder = r'C:\\Users\\ADMIN\\project soft computing\\IPDataForGlioma'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64 for faster training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to range [-1, 1]\n",
    "])\n",
    "brain_tumor_dataset = BrainTumorDataset(data_folder, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 64  # Adjust batch size according to your preference and resources\n",
    "train_loader = DataLoader(brain_tumor_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Assuming you are generating 512x512 RGB images\n",
    "channels = 3\n",
    "image_size = 512\n",
    "input_size = channels * image_size * image_size\n",
    "\n",
    "# Discriminator hyperparameters\n",
    "d_hidden_size = 32\n",
    "d_output_size = 1\n",
    "\n",
    "# Generator hyperparameters\n",
    "z_size = 100\n",
    "g_hidden_size = 32\n",
    "g_output_size = channels * image_size * image_size\n",
    "\n",
    "# Instantiate the generator\n",
    "G = Generator(z_size, g_hidden_size, g_output_size)\n",
    "\n",
    "# Instantiate the discriminator\n",
    "D = Discriminator(input_size, d_hidden_size, d_output_size)\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002  # Reduce the learning rate for stability\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100  # Increase the number of epochs for better convergence\n",
    "sample_size = 16\n",
    "fixed_z = torch.randn(sample_size, z_size, 1, 1)\n",
    "\n",
    "# Training loop\n",
    "G.train()\n",
    "D.train()\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "output_dir = r'C:\\Users\\ADMIN\\project soft computing\\new_generated_images_glioma'\n",
    "\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_i, real_images in enumerate(train_loader):\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Move the real images to the selected device\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # Train the discriminator with real images\n",
    "        d_optimizer.zero_grad()\n",
    "        real_labels = torch.ones((batch_size, 1, 1, 1), device=device)  # Labels for real images (1)\n",
    "        real_outputs = D(real_images)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_real.backward(retain_graph=True)\n",
    "\n",
    "        # Train the discriminator with fake images generated by the generator\n",
    "        z = torch.randn(batch_size, z_size, 1, 1, device=device)\n",
    "        fake_images = G(z)\n",
    "        fake_labels = torch.zeros((batch_size, 1, 1, 1), device=device)  # Labels for fake images (0)\n",
    "        fake_outputs = D(fake_images.detach())  # Detach fake_images from the generator to prevent gradients from propagating to G\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss_fake.backward()\n",
    "\n",
    "\n",
    "        # Calculate gradient penalty\n",
    "        gradient_penalty = calculate_gradient_penalty(D, real_images, fake_images, device)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        # Combine the losses and update discriminator's parameters\n",
    "        d_loss = d_loss_real + d_loss_fake + gradient_penalty\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1)  # Labels for fake images (1) since we want to fool the discriminator\n",
    "        fake_outputs = D(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print the losses\n",
    "        if batch_i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] | Batch [{batch_i + 1}/{len(train_loader)}] | Gen Loss: {g_loss.item():.4f} | Disc Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "    # Save generator and discriminator losses for plotting\n",
    "    losses_g.append(g_loss.item())\n",
    "    losses_d.append(d_loss.item())\n",
    "\n",
    "    # Generate and save sample images every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            samples_z = G(fixed_z)\n",
    "        # Rescale generated images back to the range [0, 1]\n",
    "        samples_z = (samples_z + 1) / 2\n",
    "\n",
    "        # Save the generated images\n",
    "        for i, sample in enumerate(samples_z):\n",
    "            image_path = os.path.join(output_dir, f\"generated_image_epoch{epoch + 1}_sample{i + 1}.png\")\n",
    "            # Reshape the tensor to 2D (64x64) and convert to grayscale image\n",
    "            sample_image = transforms.ToPILImage()(sample.view(64, 64).cpu())\n",
    "            sample_image.save(image_path)\n",
    "\n",
    "        G.train()       \n",
    "        \n",
    "# Plot the generator and discriminator losses\n",
    "plt.plot(losses_g, label=\"Generator Loss\")\n",
    "plt.plot(losses_d, label=\"Discriminator Loss\")\n",
    "plt.title(\"Generator and Discriminator Losses\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "989c20ef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20132659200 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\2525117497.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;31m# Instantiate the generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m \u001b[0mG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_hidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_output_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;31m# Instantiate the discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2376\\2525117497.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, z_size, channels, hidden_size)\u001b[0m\n\u001b[0;32m     18\u001b[0m         self.main = nn.Sequential(\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m# Input: z_size x 1 x 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBatchNorm2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias, dilation, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mdilation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0moutput_padding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_padding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             True, output_padding, groups, bias, padding_mode, **factory_kwargs)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[0mfactory_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'device'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dtype'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    626\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_padding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             self.weight = Parameter(torch.empty(\n\u001b[0m\u001b[0;32m    135\u001b[0m                 (in_channels, out_channels // groups, *kernel_size), **factory_kwargs))\n\u001b[0;32m    136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 20132659200 bytes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Generator class\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_size, channels, hidden_size=16):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: z_size x 1 x 1\n",
    "            nn.ConvTranspose2d(z_size, hidden_size * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 4),\n",
    "            nn.ReLU(True),\n",
    "            # Output: (hidden_size*4) x 4 x 4\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size * 4, hidden_size * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size * 2),\n",
    "            nn.ReLU(True),\n",
    "            # Output: (hidden_size*2) x 8 x 8\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size * 2, hidden_size, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_size),\n",
    "            nn.ReLU(True),\n",
    "            # Output: hidden_size x 16 x 16\n",
    "\n",
    "            nn.ConvTranspose2d(hidden_size, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: channels x 32 x 32 (final image size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "# Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    " \n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim*4)\n",
    "        self.fc2 = nn.Linear(hidden_dim*4, hidden_dim*2)\n",
    "        self.fc3 = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = self.dropout(x)\n",
    "        out = torch.sigmoid(self.fc4(x))  # Use sigmoid activation to ensure output in the range [0, 1]\n",
    "        return out.view(-1, 1, 1, 1)  # Reshape the output to (batch_size, 1, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "# Custom Dataset and transformation\n",
    "class BrainTumorDataset(Dataset):\n",
    "    def __init__(self, data_folder, transform=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.image_list = os.listdir(data_folder)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.data_folder, self.image_list[idx])\n",
    "        image = Image.open(image_path).convert(\"L\")  # Convert to grayscale image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "data_folder = r'C:\\Users\\ADMIN\\project soft computing\\IPDataForGlioma'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # Resize to 64x64 for faster training\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),  # Normalize to range [-1, 1]\n",
    "])\n",
    "brain_tumor_dataset = BrainTumorDataset(data_folder, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32  # Adjust batch size according to your preference and resources\n",
    "train_loader = DataLoader(brain_tumor_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Assuming you are generating 512x512 RGB images\n",
    "channels = 3\n",
    "image_size = 512\n",
    "input_size = channels * image_size * image_size\n",
    "\n",
    "# Discriminator hyperparameters\n",
    "hidden_dim = 16\n",
    "d_output_size = 1\n",
    "\n",
    "# Generator hyperparameters\n",
    "z_size = 100\n",
    "g_hidden_size = 16\n",
    "g_output_size = channels * image_size * image_size\n",
    "\n",
    "# Instantiate the generator\n",
    "G = Generator(z_size, g_hidden_size, g_output_size)\n",
    "\n",
    "# Instantiate the discriminator\n",
    "D = Discriminator(input_size, hidden_dim, d_output_size)\n",
    "\n",
    "# Optimizers\n",
    "lr = 0.0002  # Reduce the learning rate for stability\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 100  # Increase the number of epochs for better convergence\n",
    "sample_size = 16\n",
    "fixed_z = torch.randn(sample_size, z_size, 1, 1)\n",
    "\n",
    "# Training loop\n",
    "G.train()\n",
    "D.train()\n",
    "losses_g = []\n",
    "losses_d = []\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "output_dir = r'C:\\Users\\ADMIN\\project soft computing\\new_generated_images_glioma'\n",
    "\n",
    "\n",
    "# Determine the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "G.to(device)\n",
    "D.to(device)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_i, real_images in enumerate(train_loader):\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # Move the real images to the selected device\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # Train the discriminator with real images\n",
    "        d_optimizer.zero_grad()\n",
    "        real_labels = torch.ones((batch_size, 1, 1, 1), device=device)  # Labels for real images (1)\n",
    "        real_outputs = D(real_images)\n",
    "        d_loss_real = criterion(real_outputs, real_labels)\n",
    "        d_loss_real.backward(retain_graph=True)\n",
    "\n",
    "        # Train the discriminator with fake images generated by the generator\n",
    "        z = torch.randn(batch_size, z_size, 1, 1, device=device)\n",
    "        fake_images = G(z)\n",
    "        fake_labels = torch.zeros((batch_size, 1, 1, 1), device=device)  # Labels for fake images (0)\n",
    "        fake_outputs = D(fake_images.detach())  # Detach fake_images from the generator to prevent gradients from propagating to G\n",
    "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
    "        d_loss_fake.backward()\n",
    "\n",
    "\n",
    "        # Calculate gradient penalty\n",
    "        gradient_penalty = calculate_gradient_penalty(D, real_images, fake_images, device)\n",
    "        gradient_penalty.backward()\n",
    "\n",
    "        # Combine the losses and update discriminator's parameters\n",
    "        d_loss = d_loss_real + d_loss_fake + gradient_penalty\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1)  # Labels for fake images (1) since we want to fool the discriminator\n",
    "        fake_outputs = D(fake_images)\n",
    "        g_loss = criterion(fake_outputs, real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print the losses\n",
    "        if batch_i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}] | Batch [{batch_i + 1}/{len(train_loader)}] | Gen Loss: {g_loss.item():.4f} | Disc Loss: {d_loss.item():.4f}\")\n",
    "\n",
    "    # Save generator and discriminator losses for plotting\n",
    "    losses_g.append(g_loss.item())\n",
    "    losses_d.append(d_loss.item())\n",
    "\n",
    "    # Generate and save sample images every few epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        G.eval()\n",
    "        with torch.no_grad():\n",
    "            samples_z = G(fixed_z)\n",
    "        # Rescale generated images back to the range [0, 1]\n",
    "        samples_z = (samples_z + 1) / 2\n",
    "\n",
    "        # Save the generated images\n",
    "        for i, sample in enumerate(samples_z):\n",
    "            image_path = os.path.join(output_dir, f\"generated_image_epoch{epoch + 1}_sample{i + 1}.png\")\n",
    "            # Reshape the tensor to 2D (64x64) and convert to grayscale image\n",
    "            sample_image = transforms.ToPILImage()(sample.view(64, 64).cpu())\n",
    "            sample_image.save(image_path)\n",
    "\n",
    "        G.train()       \n",
    "        \n",
    "# Plot the generator and discriminator losses\n",
    "plt.plot(losses_g, label=\"Generator Loss\")\n",
    "plt.plot(losses_d, label=\"Discriminator Loss\")\n",
    "plt.title(\"Generator and Discriminator Losses\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f9bdfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting swat\n",
      "  Downloading swat-1.13.2-0-cp39-cp39-win_amd64.whl (54.1 MB)\n",
      "     -------------------------------------- 54.1/54.1 MB 925.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (from swat) (1.26.11)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from swat) (1.16.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from swat) (2.28.1)\n",
      "Requirement already satisfied: pandas>=0.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from swat) (1.4.4)\n",
      "Requirement already satisfied: pytz in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from swat) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.16.0->swat) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\admin\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.16.0->swat) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->swat) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->swat) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->swat) (2022.9.14)\n",
      "Installing collected packages: swat\n",
      "Successfully installed swat-1.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install swat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aadc18de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635a7e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SWATError",
     "evalue": "Could not find a suitable TLS CA certificate bundle, invalid path: C:\\Program Files\\Common Files\\ssl/cert.pem",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\rest\\connection.py\u001b[0m in \u001b[0;36m_connect\u001b[1;34m(self, session, locale, wait_until_idle)\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[0mput_retries\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mconnection_retries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_req_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m502\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mput\u001b[1;34m(self, url, data, **kwargs)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PUT\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcert_verify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcert\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36mcert_verify\u001b[1;34m(self, conn, url, verify, cert)\u001b[0m\n\u001b[0;32m    262\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcert_loc\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcert_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                 raise OSError(\n\u001b[0m\u001b[0;32m    264\u001b[0m                     \u001b[1;34mf\"Could not find a suitable TLS CA certificate bundle, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Could not find a suitable TLS CA certificate bundle, invalid path: C:\\Program Files\\Common Files\\ssl/cert.pem",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSWATError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11628\\4191214980.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m conn = swat.CAS(hostname=\"https://www.sas.com/en_in/trials/software/viya/viya-trial-form.html\",\n\u001b[0m\u001b[0;32m      2\u001b[0m               port=5570, username=\"Pratik bharadwaj\", password=\"Singh@9939\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\connection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hostname, port, username, password, session, locale, nworkers, name, authinfo, protocol, path, ssl_ca_list, authcode, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhostname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musername\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msoptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sw_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'http'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'https'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sw_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREST_CASConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sw_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSW_CASConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\rest\\connection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hostname, port, username, password, soptions, error)\u001b[0m\n\u001b[0;32m    341\u001b[0m             })\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlocale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait_until_idle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwait_until_idle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\rest\\connection.py\u001b[0m in \u001b[0;36m_connect\u001b[1;34m(self, session, locale, wait_until_idle)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mSWATError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwait_until_idle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSWATError\u001b[0m: Could not find a suitable TLS CA certificate bundle, invalid path: C:\\Program Files\\Common Files\\ssl/cert.pem"
     ]
    }
   ],
   "source": [
    "conn = swat.CAS(hostname=\"https://www.sas.com/en_in/trials/software/viya/viya-trial-form.html\",\n",
    "              port=5570, username=\"Pratik bharadwaj\", password=\"Singh@9939\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518bef8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Failed to connect to host 'azureuse011696.my-trials.sas.com', port 11696.\n"
     ]
    },
    {
     "ename": "SWATError",
     "evalue": "Could not connect to 'azureuse011696.my-trials.sas.com' on port 11696.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\connection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hostname, port, username, password, session, locale, nworkers, name, authinfo, protocol, path, ssl_ca_list, authcode, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sw_connection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSW_CASConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\clib.py\u001b[0m in \u001b[0;36mSW_CASConnection\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0m_import_pyswat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pyswat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSW_CASConnection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class 'py39swat.SW_CASConnection'> returned NULL without setting an error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSWATError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11628\\4222033401.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m conn = swat.CAS(hostname=\"azureuse011696.my-trials.sas.com\",  # Replace with your SAS Viya URL\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[0mport\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11696\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Default CAS port for SAS Viya\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[0musername\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Pratik bharadwaj\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 password=\"Singh@9939\")\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\swat\\cas\\connection.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, hostname, port, username, password, session, locale, nworkers, name, authinfo, protocol, path, ssl_ca_list, authcode, **kwargs)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mSWATError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sw_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLastErrorMessage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[1;31m# Set up index origin for error messages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSWATError\u001b[0m: Could not connect to 'azureuse011696.my-trials.sas.com' on port 11696."
     ]
    }
   ],
   "source": [
    "conn = swat.CAS(hostname=\"azureuse011696.my-trials.sas.com\",  # Replace with your SAS Viya URL\n",
    "                port=11696,  # Default CAS port for SAS Viya\n",
    "                username=\"Pratik bharadwaj\",\n",
    "                password=\"Singh@9939\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
